"""
Scrapers espec√≠ficos para diferentes lojas online.
Cada loja tem sua pr√≥pria l√≥gica de extra√ß√£o otimizada.
"""
import requests
from bs4 import BeautifulSoup
import re
import logging
from urllib.parse import urlparse

logger = logging.getLogger(__name__)


class ScrapingError(Exception):
    """Excecao base para erros de scraping"""
    pass


class NetworkError(ScrapingError):
    """Erro de rede/HTTP (timeout, 404, 500, etc.) - NAO gera issue"""
    pass


class ParsingError(ScrapingError):
    """Erro de parsing/extracao de dados (site acessivel mas dados nao extraidos) - GERA issue"""
    pass


class BaseScraper:
    """Classe base para scrapers de lojas"""

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',
        }

    def get_soup(self, url, timeout=10):
        """
        Obt√©m BeautifulSoup de uma URL.

        Lanca NetworkError para erros HTTP/rede (404, 500, timeout, etc.)
        Retorna BeautifulSoup se sucesso.
        """
        try:
            response = requests.get(url, headers=self.headers, timeout=timeout)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except requests.exceptions.Timeout as e:
            logger.error(f"Timeout ao acessar {url}: {str(e)}")
            raise NetworkError(f"Timeout ao acessar URL: {str(e)}")
        except requests.exceptions.HTTPError as e:
            # Erros HTTP (404, 500, etc.)
            logger.error(f"Erro HTTP ao acessar {url}: {str(e)}")
            raise NetworkError(f"Erro HTTP {e.response.status_code}: {str(e)}")
        except requests.exceptions.ConnectionError as e:
            logger.error(f"Erro de conexao ao acessar {url}: {str(e)}")
            raise NetworkError(f"Erro de conexao: {str(e)}")
        except requests.exceptions.RequestException as e:
            # Outros erros de request
            logger.error(f"Erro de rede ao acessar {url}: {str(e)}")
            raise NetworkError(f"Erro de rede: {str(e)}")
        except Exception as e:
            # Erros inesperados (parsing HTML, etc.)
            logger.error(f"Erro inesperado ao obter p√°gina {url}: {str(e)}")
            raise NetworkError(f"Erro inesperado: {str(e)}")

    def clean_price(self, price_str):
        """Limpa e converte string de pre√ßo para float"""
        if not price_str:
            return None

        # Remove tudo exceto n√∫meros, v√≠rgula e ponto
        price_str = re.sub(r'[^\d,.]', '', price_str)

        # Substitui v√≠rgula por ponto
        price_str = price_str.replace('.', '').replace(',', '.')

        try:
            return float(price_str)
        except:
            return None

    def extract(self, url):
        """M√©todo a ser implementado por subclasses"""
        raise NotImplementedError


class AmazonScraper(BaseScraper):
    """Scraper espec√≠fico para Amazon Brasil"""

    def __init__(self):
        super().__init__()
        # Headers mais completos para Amazon
        self.headers.update({
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.amazon.com.br/',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })

    def extract(self, url):
        """
        Extrai informa√ß√µes de produtos da Amazon.
        Retorna: (titulo, preco, imagem_url)
        Lanca NetworkError para erros de rede/HTTP.
        Lanca ParsingError se conseguir acessar mas nao extrair dados minimos.
        """
        # get_soup() pode lancar NetworkError (propagamos)
        soup = self.get_soup(url)

        titulo = None
        preco = None
        imagem_url = None

        # T√≠tulo - Amazon tem IDs espec√≠ficos + mais alternativas
        title_candidates = [
            soup.find('span', id='productTitle'),
            soup.find('h1', id='title'),
            soup.find('h1', class_='a-size-large'),
            soup.find('span', {'class': 'product-title-word-break'}),
            soup.find('h1', {'class': 'a-size-large a-spacing-none'}),
        ]

        for candidate in title_candidates:
            if candidate:
                titulo = candidate.get_text(strip=True)
                if titulo:  # Verifica se n√£o est√° vazio
                    break

        # Pre√ßo - Amazon tem v√°rias possibilidades + mais alternativas
        price_candidates = [
            # Pre√ßo principal
            soup.find('span', class_='a-price-whole'),
            # Pre√ßo de oferta
            soup.find('span', class_='a-offscreen'),
            # Pre√ßo no deal
            soup.find('span', class_='priceBlockBuyingPriceString'),
            # Pre√ßo atual
            soup.find('span', {'class': 'a-price aok-align-center reinventPricePriceToPayMargin priceToPay'}),
            # Pre√ßo no elemento data
            soup.find('span', {'data-a-color': 'price'}),
            # Pre√ßo em divs espec√≠ficas
            soup.select_one('.a-price .a-offscreen'),
            soup.select_one('#priceblock_ourprice'),
            soup.select_one('#priceblock_dealprice'),
        ]

        for candidate in price_candidates:
            if candidate:
                price_text = candidate.get_text(strip=True)
                preco = self.clean_price(price_text)
                if preco:
                    break

        # Imagem - Amazon tem imgTagWrapper + mais alternativas
        img_candidates = [
            soup.find('img', id='landingImage'),
            soup.find('img', class_='a-dynamic-image'),
            soup.find('div', id='imgTagWrapperId'),
            soup.find('img', {'data-a-image-name': 'landingImage'}),
            soup.select_one('#imageBlock img'),
            soup.select_one('.imgTagWrapper img'),
        ]

        for candidate in img_candidates:
            if candidate:
                if candidate.name == 'img':
                    imagem_url = candidate.get('src') or candidate.get('data-old-hires') or candidate.get('data-a-dynamic-image')
                    # Se data-a-dynamic-image √© um JSON, pegar a primeira URL
                    if imagem_url and imagem_url.startswith('{'):
                        try:
                            import json
                            images = json.loads(imagem_url)
                            if images:
                                imagem_url = list(images.keys())[0]
                        except:
                            pass
                else:
                    img = candidate.find('img')
                    if img:
                        imagem_url = img.get('src') or img.get('data-old-hires') or img.get('data-a-dynamic-image')

                if imagem_url and imagem_url.startswith('http'):
                    break

        # Log do resultado detalhado (vis√≠vel no console Render)
        logger.info("=" * 80)
        logger.info(f"üõçÔ∏è  AMAZON SCRAPING - URL: {url[:80]}...")
        logger.info(f"   üìù T√≠tulo:  {'‚úÖ Extra√≠do' if titulo else '‚ùå FALHOU'} - {titulo[:60] if titulo else 'N/A'}...")
        logger.info(f"   üí∞ Pre√ßo:   {'‚úÖ Extra√≠do' if preco else '‚ö†Ô∏è  N√£o encontrado'} - R$ {preco if preco else 'N/A'}")
        logger.info(f"   üñºÔ∏è  Imagem:  {'‚úÖ Extra√≠da' if imagem_url else '‚ö†Ô∏è  N√£o encontrada'}")
        logger.info("=" * 80)

        # Se nao conseguiu extrair NENHUM titulo, lancar ParsingError
        if not titulo:
            # Log de erro cr√≠tico (vis√≠vel no Render)
            logger.error("!" * 80)
            logger.error("‚ùå ERRO CR√çTICO DE SCRAPING - AMAZON")
            logger.error(f"   URL: {url}")
            logger.error(f"   T√≠tulo: N√£o extra√≠do ‚ùå")
            logger.error(f"   Pre√ßo: {f'R$ {preco}' if preco else 'N√£o extra√≠do'}")
            logger.error(f"   Imagem: {'Extra√≠da ‚úÖ' if imagem_url else 'N√£o extra√≠da'}")
            logger.error("   ")
            logger.error("   ‚ö†Ô∏è  ATEN√á√ÉO: Issue ser√° criada automaticamente no GitHub")
            logger.error("!" * 80)

            # Log HTML para debug (primeiros 1000 chars)
            html_snippet = str(soup)[:1000] if soup else 'N/A'
            logger.debug(f"Amazon ParsingError - HTML snippet: {html_snippet}")

            raise ParsingError(f"Nao foi possivel extrair titulo da Amazon. Dados parciais: preco={preco}, imagem={bool(imagem_url)}")

        return (titulo, preco, imagem_url)


class MercadoLivreScraper(BaseScraper):
    """Scraper espec√≠fico para Mercado Livre"""

    def extract(self, url):
        """
        Extrai informa√ß√µes de produtos do Mercado Livre.
        Retorna: (titulo, preco, imagem_url)
        Lanca NetworkError para erros de rede/HTTP.
        Lanca ParsingError se conseguir acessar mas nao extrair dados minimos.
        """
        # get_soup() pode lancar NetworkError (propagamos)
        soup = self.get_soup(url)

        titulo = None
        preco = None
        imagem_url = None

        # T√≠tulo - Mercado Livre usa h1.ui-pdp-title
        title_candidates = [
            soup.find('h1', class_='ui-pdp-title'),
            soup.find('h1', class_=re.compile('title')),
        ]

        for candidate in title_candidates:
            if candidate:
                titulo = candidate.get_text(strip=True)
                break

        # Pre√ßo - Mercado Livre tem classes espec√≠ficas
        price_candidates = [
            soup.find('span', class_='andes-money-amount__fraction'),
            soup.find('span', class_=re.compile('price-tag-fraction')),
            soup.find('meta', property='product:price:amount'),
        ]

        for candidate in price_candidates:
            if candidate:
                if candidate.name == 'meta':
                    price_text = candidate.get('content', '')
                else:
                    price_text = candidate.get_text(strip=True)

                preco = self.clean_price(price_text)
                if preco:
                    break

        # Imagem - Mercado Livre tem figura principal
        img_candidates = [
            soup.find('figure', class_='ui-pdp-gallery__figure'),
            soup.find('img', class_='ui-pdp-image'),
            soup.find('meta', property='og:image'),
        ]

        for candidate in img_candidates:
            if candidate:
                if candidate.name == 'meta':
                    imagem_url = candidate.get('content')
                elif candidate.name == 'img':
                    imagem_url = candidate.get('src') or candidate.get('data-src')
                else:
                    img = candidate.find('img')
                    if img:
                        imagem_url = img.get('src') or img.get('data-src')

                if imagem_url:
                    break

        # Log do resultado
        logger.info(f"Mercado Livre - T√≠tulo: {bool(titulo)}, Pre√ßo: {preco}, Imagem: {bool(imagem_url)}")

        # Se nao conseguiu extrair NENHUM titulo, lancar ParsingError
        if not titulo:
            raise ParsingError(f"Nao foi possivel extrair titulo do Mercado Livre. Dados parciais: preco={preco}, imagem={bool(imagem_url)}")

        return (titulo, preco, imagem_url)


class KabumScraper(BaseScraper):
    """Scraper espec√≠fico para Kabum"""

    def extract(self, url):
        """
        Extrai informa√ß√µes de produtos do Kabum.
        Retorna: (titulo, preco, imagem_url)
        Lanca NetworkError para erros de rede/HTTP.
        Lanca ParsingError se conseguir acessar mas nao extrair dados minimos.
        """
        # get_soup() pode lancar NetworkError (propagamos)
        soup = self.get_soup(url)

        titulo = None
        preco = None
        imagem_url = None

        # T√≠tulo
        title_candidates = [
            soup.find('h1', class_=re.compile('title|product')),
            soup.find('h1'),
        ]

        for candidate in title_candidates:
            if candidate:
                titulo = candidate.get_text(strip=True)
                break

        # Pre√ßo
        price_candidates = [
            soup.find('span', class_=re.compile('price|preco')),
            soup.find('h4', class_=re.compile('price|preco')),
        ]

        for candidate in price_candidates:
            if candidate:
                price_text = candidate.get_text(strip=True)
                preco = self.clean_price(price_text)
                if preco:
                    break

        # Imagem
        imagem_url = None
        og_image = soup.find('meta', property='og:image')
        if og_image:
            imagem_url = og_image.get('content')

        logger.info(f"Kabum - T√≠tulo: {bool(titulo)}, Pre√ßo: {preco}, Imagem: {bool(imagem_url)}")

        # Se nao conseguiu extrair NENHUM titulo, lancar ParsingError
        if not titulo:
            raise ParsingError(f"Nao foi possivel extrair titulo do Kabum. Dados parciais: preco={preco}, imagem={bool(imagem_url)}")

        return (titulo, preco, imagem_url)


class GenericScraper(BaseScraper):
    """Scraper gen√©rico para qualquer loja"""

    def extract(self, url):
        """
        Extrai informa√ß√µes usando meta tags gen√©ricas.
        Retorna: (titulo, preco, imagem_url)
        Lanca NetworkError para erros de rede/HTTP.
        Lanca ParsingError se conseguir acessar mas nao extrair dados minimos.
        """
        # get_soup() pode lancar NetworkError (propagamos)
        soup = self.get_soup(url)

        titulo = None
        preco = None
        imagem_url = None

        # T√≠tulo - tentar meta tags primeiro
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            titulo = og_title.get('content')

        if not titulo:
            twitter_title = soup.find('meta', attrs={'name': 'twitter:title'})
            if twitter_title and twitter_title.get('content'):
                titulo = twitter_title.get('content')

        if not titulo and soup.title:
            titulo = soup.title.string

        if not titulo:
            h1 = soup.find('h1')
            if h1:
                titulo = h1.get_text(strip=True)

        if titulo:
            titulo = re.sub(r'\s*[\|\-]\s*[A-Za-z0-9\s]+$', '', titulo)
            titulo = titulo.strip()[:200]

        # Pre√ßo - tentar v√°rias estrat√©gias
        price_patterns = [
            r'R\$?\s*(\d+(?:[.,]\d{3})*(?:[.,]\d{2}))',
            r'(\d+(?:[.,]\d{3})*(?:[.,]\d{2}))\s*reais?',
        ]

        price_meta = soup.find('meta', property='product:price:amount') or soup.find('meta', attrs={'itemprop': 'price'})
        if price_meta and price_meta.get('content'):
            preco = self.clean_price(price_meta.get('content'))

        if not preco:
            price_elements = soup.find_all(['span', 'div', 'strong', 'p'], class_=re.compile(r'price|preco|valor|value', re.I))
            for elem in price_elements:
                text = elem.get_text(strip=True)
                for pattern in price_patterns:
                    match = re.search(pattern, text, re.I)
                    if match:
                        preco = self.clean_price(match.group(1))
                        if preco:
                            break
                if preco:
                    break

        # Imagem
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            imagem_url = og_image.get('content')

        if not imagem_url:
            twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
            if twitter_image and twitter_image.get('content'):
                imagem_url = twitter_image.get('content')

        if imagem_url and not imagem_url.startswith('http'):
            from urllib.parse import urljoin
            imagem_url = urljoin(url, imagem_url)

        logger.info(f"Generic - T√≠tulo: {bool(titulo)}, Pre√ßo: {preco}, Imagem: {bool(imagem_url)}")

        # Se nao conseguiu extrair NENHUM titulo, lancar ParsingError
        if not titulo:
            raise ParsingError(f"Nao foi possivel extrair titulo (scraper generico). Dados parciais: preco={preco}, imagem={bool(imagem_url)}")

        return (titulo, preco, imagem_url)


class ScraperFactory:
    """F√°brica para selecionar o scraper apropriado baseado na URL"""

    @staticmethod
    def get_scraper(url):
        """
        Retorna o scraper apropriado baseado na URL.
        Retorna: (scraper, is_generic)
        """
        domain = urlparse(url).netloc.lower()

        if 'amazon.com' in domain:
            logger.info(f"üõçÔ∏è  Usando AmazonScraper para {domain}")
            return AmazonScraper(), False
        elif 'mercadolivre.com' in domain or 'mercadolibre.com' in domain:
            logger.info(f"üõí Usando MercadoLivreScraper para {domain}")
            return MercadoLivreScraper(), False
        elif 'kabum.com' in domain:
            logger.info(f"üéÆ Usando KabumScraper para {domain}")
            return KabumScraper(), False
        else:
            logger.warning("=" * 80)
            logger.warning(f"‚ö†Ô∏è  SITE N√ÉO MAPEADO: {domain}")
            logger.warning(f"   URL: {url}")
            logger.warning(f"   ")
            logger.warning(f"   ‚ÑπÔ∏è  Usando scraper gen√©rico (pode ter menor taxa de sucesso)")
            logger.warning(f"   üí° Considere adicionar suporte espec√≠fico para este site")
            logger.warning("=" * 80)
            return GenericScraper(), True

    @staticmethod
    def extract_product_info(url):
        """
        Extrai informa√ß√µes de produto usando o scraper apropriado.

        Retorna:
            - Em caso de sucesso: dict com {
                'success': True,
                'titulo': str,
                'preco': float or None,
                'imagem_url': str or None
              }
            - Em caso de erro de rede/HTTP: dict com {
                'success': False,
                'error_type': 'network',
                'error_message': str
              }
            - Em caso de erro de parsing: dict com {
                'success': False,
                'error_type': 'parsing',
                'error_message': str,
                'partial_data': {'titulo': ..., 'preco': ..., 'imagem_url': ...}
              }
        """
        try:
            scraper, is_generic = ScraperFactory.get_scraper(url)
            titulo, preco, imagem_url = scraper.extract(url)

            # Se usou scraper gen√©rico e teve sucesso, criar issue sugerindo suporte espec√≠fico
            if is_generic:
                try:
                    from .github_helper import criar_issue_erro_geral
                    from urllib.parse import urlparse

                    domain = urlparse(url).netloc
                    titulo_issue = f"[AUTO] Site n√£o mapeado: {domain}"
                    descricao_issue = f"""## Site N√£o Mapeado Detectado

O sistema conseguiu extrair dados usando o **scraper gen√©rico**, mas este site n√£o tem suporte espec√≠fico implementado.

### Dom√≠nio
`{domain}`

### URL de Exemplo
```
{url}
```

### Dados Extra√≠dos com Scraper Gen√©rico
- **T√≠tulo**: {titulo[:100] if titulo else 'N/A'}...
- **Pre√ßo**: R$ {preco if preco else 'N/A'}
- **Imagem**: {'Sim' if imagem_url else 'N√£o'}

### Por Que Adicionar Suporte Espec√≠fico?
1. **Maior taxa de sucesso**: Scrapers espec√≠ficos conhecem a estrutura exata do site
2. **Mais dados**: Podem extrair informa√ß√µes adicionais (descri√ß√£o, avalia√ß√µes, etc.)
3. **Mais confi√°vel**: Menos propenso a falhas quando o site muda minimamente
4. **Melhor performance**: Busca direta nos elementos corretos

### A√ß√µes Sugeridas
- [ ] Analisar estrutura HTML do site `{domain}`
- [ ] Identificar padr√µes de classes CSS para t√≠tulo, pre√ßo, imagem
- [ ] Criar classe `{domain.split('.')[0].title()}Scraper` em `scrapers.py`
- [ ] Implementar m√©todo `extract()` espec√≠fico
- [ ] Adicionar ao `ScraperFactory.get_scraper()`
- [ ] Testar com m√∫ltiplas URLs do site

### Sites Populares que Merecem Suporte
Se este for um dos sites abaixo, priorize a implementa√ß√£o:
- Americanas
- Submarino
- Magazine Luiza
- Casas Bahia
- Shopee
- AliExpress
- Shein
- Netshoes
- Centauro

### Exemplo de Implementa√ß√£o
```python
class {domain.split('.')[0].title()}Scraper(BaseScraper):
    def extract(self, url):
        soup = self.get_soup(url)

        # T√≠tulo
        titulo = soup.find('h1', class_='product-title')

        # Pre√ßo
        preco = soup.find('span', class_='price')

        # Imagem
        imagem = soup.find('img', class_='product-image')

        return (titulo, preco, imagem)
```

### Informa√ß√µes T√©cnicas
- **Tipo**: Melhoria (Enhancement)
- **Prioridade**: M√©dia (se site popular) / Baixa (se site raro)
- **Complexidade**: Baixa a M√©dia
"""

                    criar_issue_erro_geral(
                        titulo=titulo_issue,
                        descricao=descricao_issue,
                        contexto={
                            'Dom√≠nio': domain,
                            'URL': url,
                            'Scraper usado': 'GenericScraper',
                            'Status': 'Sucesso com scraper gen√©rico'
                        },
                        labels=['auto-generated', 'enhancement', 'new-site-support', 'low-priority']
                    )

                    logger.info(f"üí° Issue criada sugerindo suporte espec√≠fico para {domain}")

                except Exception as e:
                    logger.debug(f"Erro ao criar issue para site n√£o mapeado: {str(e)}")

            return {
                'success': True,
                'titulo': titulo,
                'preco': preco,
                'imagem_url': imagem_url,
                'used_generic_scraper': is_generic
            }

        except NetworkError as e:
            # Erro de rede/HTTP (404, 500, timeout, etc.)
            # NAO deve gerar issue no GitHub
            logger.warning(f"Erro de rede ao extrair {url}: {str(e)}")
            return {
                'success': False,
                'error_type': 'network',
                'error_message': str(e)
            }

        except ParsingError as e:
            # Erro de parsing (site acessivel mas dados nao extraidos)
            # DEVE gerar issue no GitHub
            logger.warning("=" * 80)
            logger.warning(f"‚ö†Ô∏è  ERRO DE PARSING ao extrair dados de: {url}")
            logger.warning(f"   Erro: {str(e)}")
            logger.warning(f"   ")
            logger.warning(f"   ‚ÑπÔ∏è  Tentando criar issue no GitHub automaticamente...")
            logger.warning("=" * 80)

            # Tentar extrair dados parciais da mensagem de erro (se houver)
            # Formato da mensagem: "Nao foi possivel extrair titulo. Dados parciais: preco=123.45, imagem=True"
            partial_data = {'titulo': None, 'preco': None, 'imagem_url': None}

            # Tentar criar issue no GitHub
            try:
                from .github_helper import criar_issue_falha_scraping

                issue_result = criar_issue_falha_scraping(
                    url_produto=url,
                    dados_extraidos=partial_data,
                    usuario=None,  # Usuario nao disponivel neste contexto
                    grupo=None
                )

                if issue_result and issue_result.get('success'):
                    logger.info(f"‚úÖ Issue #{issue_result['issue_number']} criada: {issue_result['issue_url']}")
                else:
                    logger.warning(f"‚ö†Ô∏è  Falha ao criar issue: {issue_result.get('error') if issue_result else 'N/A'}")

            except Exception as issue_error:
                logger.error(f"‚ùå Erro ao tentar criar issue no GitHub: {str(issue_error)}")

            return {
                'success': False,
                'error_type': 'parsing',
                'error_message': str(e),
                'partial_data': partial_data
            }

        except Exception as e:
            # Outros erros inesperados
            logger.error(f"Erro inesperado ao extrair {url}: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())

            return {
                'success': False,
                'error_type': 'unknown',
                'error_message': str(e)
            }
